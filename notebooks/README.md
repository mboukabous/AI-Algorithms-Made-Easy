# Notebooks

This folder contains Jupyter notebooks designed to demonstrate, experiment, and analyze various machine learning workflows. Each notebook is a standalone resource and includes detailed explanations and code for end-to-end machine learning tasks.

## Contents

### 1. [Train_Supervised_Regression_Models.ipynb](Train_Supervised_Regression_Models.ipynb)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mboukabous/AI-Algorithms-Made-Easy/blob/main/notebooks/Train_Supervised_Regression_Models.ipynb)

#### Overview
This notebook focuses on training supervised regression models using Python and popular machine learning libraries. It is designed to guide users through the following steps:

1. **Dataset Management**:
   - Instructions for downloading datasets directly from Kaggle or using your dataset for training.

2. **Preprocessing**:
   - Comprehensive data preprocessing using pipelines, including handling missing values, scaling numerical features, and encoding categorical features.

3. **Model Training**:
   - Step-by-step training of various regression models:
     - Linear Regression
     - Ridge Regression
     - Lasso Regression
     - ElasticNet Regression
     - Decision Tree
     - Random Forest (Bagging)
     - Gradient Boosting (Boosting)
     - AdaBoost (Boosting)
     - XGBoost (Boosting)
     - LightGBM
     - CatBoost
     - Support Vector Regressor (SVR)
     - K-Nearest Neighbors (KNN) Regressor
     - Extra Trees Regressor
     - Multilayer Perceptron (MLP) Regressor

4. **Hyperparameter Tuning**:
   - Demonstrates the use of GridSearchCV for efficient hyperparameter optimization.

5. **Results**:
   - Saving trained models, metrics, and other outputs for further analysis.

---

### 2. [Train_Supervised_Classification_Models.ipynb](Train_Supervised_Classification_Models.ipynb)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mboukabous/AI-Algorithms-Made-Easy/blob/main/notebooks/Train_Supervised_Classification_Models.ipynb)

#### Overview
This notebook provides an in-depth guide to training supervised classification models. The workflow includes:

1. **Dataset Management**:
   - How to use pre-existing classification datasets from Kaggle or your own custom dataset.

2. **Preprocessing**:
   - Handling imbalanced data, encoding categorical features, and scaling numerical features.

3. **Model Training**:
   - Training popular classification models, including:
     - Logistic Regression
     - Decision Tree Classifier
     - Random Forest Classifier
     - Gradient Boosting (Boosting)
     - AdaBoost Classifier (Boosting)
     - XGBoost Classifier (Boosting)
     - LightGBM Classifier
     - CatBoost Classifier
     - Support Vector Classifier (SVC)
     - K-Nearest Neighbors (KNN) Classifier
     - Extra Trees Classifier
     - Multilayer Perceptron (MLP) Classifier
     - GaussianNB (Naive Bayes Classifier)
     - Linear Discriminant Analysis (LDA)
     - Quadratic Discriminant Analysis (QDA)

4. **Hyperparameter Tuning**:
   - Shows how to use GridSearchCV for hyperparameter tuning to optimize model performance.

5. **Results and Evaluation**:
   - Metrics such as accuracy, precision, recall, F1-score, and confusion matrix are visualized for easy interpretation.

---

### 3. [Train_Unsupervised_Models.ipynb](Train_Unsupervised_Models.ipynb)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mboukabous/AI-Algorithms-Made-Easy/blob/main/notebooks/Train_Unsupervised_Models.ipynb)

#### Overview
This notebook focuses on unsupervised learning workflows for clustering, dimensionality reduction, and anomaly detection. Steps include:

1. **Clustering**:
   - Explore various clustering algorithms such as:
     - KMeans
     - DBSCAN
     - Gaussian Mixture Models
     - Agglomerative Clustering

2. **Dimensionality Reduction**:
   - Reduce the dimensionality of high-dimensional datasets using:
     - PCA
     - t-SNE
     - UMAP

3. **Anomaly Detection**:
   - Identify outliers and anomalies using:
     - Isolation Forest
     - One-Class SVM
     - Local Outlier Factor (LOF)

4. **Evaluation**:
   - Use metrics like silhouette score for clustering and visualization for dimensionality reduction and anomaly detection.

---

## Contributing
Feel free to contribute additional notebooks to this directory! Whether it's for advanced classification techniques, deep learning, or feature engineering, all contributions are welcome.

## License
This project is licensed under the MIT License.
